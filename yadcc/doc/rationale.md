# 基本原理

我们主要通过如下途径方式进行编译加速：

- 本地预处理、集群编译。相对于代码生成、优化等的计算成本，预处理通常可以很快的完成。通过将速度较慢的代码生成等逻辑分发至编译集群，可以提高编译的并行度。

  在我们的测试中，AMD EPYC (Rome)机型上GCC8.2使用`-E -fdirectives-only`参数进行预处理的代码产出速度大约为100MB/s。而同等数量（100MB）的预处理结果的代码生成、优化等大约需要2分钟。

- 分布式编译缓存。对于基于大仓库的开发模式而言，由不同人重复编译相同代码的场景是较为常见的。我们通过使用集中的编译缓存，允许不同机器上的编译任务共享编译结果来加速。

  比如用户1本地编译了代码之后合入master，之后用户2拉取代码再次编译时就有可能（取决于本地的环境等多种因素）复用之前的编译结果，节省时间。

当然分布式编译并非没有其缺陷：

- 从经验来讲，预处理后再编译的耗时总和要大于直接编译。
- 分布式编译过程中无论是多次运行编译器又或是（为了减少网络带宽）压缩等操作均有以毫秒（某些情况下可能以十毫秒）为单位的时间开销。
- 部分编译机性能过差（或突发的相对于`yadcc`的外界负载过高）导致分发上去的任务编译缓慢，成为关键路径拖慢整体编译。
- 分发至编译机引入的网络延迟。

但是对于我们的实际场景（单个TU编译时间往往数十秒，`.proto`依赖较重的TU通常超过1分钟）而言，通过分布式提高并行度带来的吞吐收益通常会明显高过由于引入延时导致的吞吐降低（吞吐 = 延迟 * 并发度）。因此，整体而言分布式编译可以有明显的正向收益。

除此之外，我们会通过本地守护进程控制并发度，将“不可分布式”的任务（如链接）的本地并发度控制在一个合理的范围内。这通常可以避免因为编译耗尽内存引发的包括但不限于如下问题：

- SSH终端断开
- [Visual Studio Code](https://code.visualstudio.com)断开

**对于非C++类编译任务，`yadcc`不提供支持。贸然使用可能会因为启动过多编译任务消耗大量内存。**

## 现有产品

我们首先调研了目前已经存在的同类产品，目前主要分为三大类。每一类有各自的优势。

- 将本地文件差异分发到高配服务器编译。

  目前我们已知的此类解决方案主要为PatchBuild。

  由于编译操作不在本地，这对本地环境的要求非常低，甚至可以是完全异构的环境。

  这儿解决的问题主要在于“异构环境”、“低配客户机”等，与分布式编译需要解决的“单机处理能力到达上限”是两个不同的问题。

  随着编译任务越发繁重，高配服务器也会存在伸缩性问题。所以接下来的一步往往是再通过分布式编译提高服务器的编译效率。

  目前我们的测试中，我们的负载在`yadcc`分布式编译时，性能在8（逻辑）核开发机上略优于90（逻辑）核机器本地编译的性能。另外，大多数用户机器配置（相对于8核开发机而言）更高。因此“使用高配服务器”收益并不大，所以暂未考虑此种设计。

  但是考虑到这种设计的固有优点（本地环境无要求），后续我们可能会根据情况（可能是单独立项）实现“提交diff到服务端”+“服务端分布式编译”来兼顾二者的优势。

- 对编译结果缓存

  这类产品主要关注重复编译的问题。通过缓存编译结果，重复编译相同文件时可以极大地改善速度。

  目前我们已知的此类解决方案主要有[`ccache`](https://ccache.dev/)、[`xcache`](https://github.com/alibaba/xcache)、[`sccache`](https://github.com/mozilla/sccache)。其中`ccache`是本地缓存，`xcache`、`sccache`是云端缓存。

  根据本地缓存或是云端缓存，各有各自需要考虑的问题。

  - 本地缓存

    在我们的实际环境中，由于[`blade`](https://github.com/chen3feng/blade-build)本身并不存在“重复编译未修改过的文件”的问题，因此实际上类似工具收益有限。

    *除非构建工具链本身存在问题，否则大多数工具链都不会重复编译未修改过（包括包含的头文件）的目标。*

    当然客观存在偶尔在两个版本之间反复修改，或`clean`后重新编译的情况，但是此类场景较少，因此对我们的收益并不明显。

  - 云端缓存

    云端缓存的场景值得我们注意。我们认为这种场景下的缓存是很有价值的。

    **云端环境下如果多个开发者使用同一个代码仓库（常见），那么一个开发者编译后`push`了代码，其他开发者`pull`下来之后，缓存是可以复用的。**

    但是另一方面，**无条件的每次查询云端缓存会引入不必要的网络开销**。取决于服务器部署，这个开销可能是不可忽略的。

- 分布式编译

  这类产品多采取本地预处理、云端编译的实现。

  我们已知的此类解决方案有[`distcc`]([distcc](https://github.com/distcc/distcc))、[`icecream`](https://github.com/icecc/icecream)。

  这类产品主要解决的是单台机器性能瓶颈的问题。

  通常而言，预处理的成本明显低于代码生成等逻辑。通过预处理将依赖众多`.h`的`.cc`合并成一个自包含的`.ii`之后，就可以非常容易的分发，实现远程编译。

  但是目前这些工具都有各自的问题：

  - `distcc`：缺少中央调度节点，工业场景下使用可能导致部分机器过载。而编译（取决于负载类型）可能是一个内存开销很大的场景，不对负载进行控制，容易OOM导致死机（CPU过载通常只会导致缓慢，不会死机。）。
  - `icecream`：比较遗憾的是`icecream`在我们的测试中问题比较多，经常hang住，简单调试了下可能跟其RPC实现有关（非预期的RPC包，比如调度器主动发出来的`dump-internals`命令，会导致守护进程状态错乱）。另外，`icecream`需要客户机的`root`权限对部署引入困难，每次编译需要（同步）请求调度器会引入延迟（尤其是广告本身是多地研发，除非各地分别部署，不然延迟难易忽略）。

  另外，目前这些工具都有如下共同的问题：

  - 不能很好的支持（分布式）编译缓存，需要用户手动再和编译缓存工具整合。这可能导致多次预处理，引入很大的性能开销。

  - 压缩算法性能问题较差，多采用`gzip`等较慢的算法。

    `ccache`和`yadcc`均采用`zstd`，改善压缩性能。

    即便如此，我们的观察中，压缩仍然占据了约15%的CPU开销。与之形成对比的是，预处理的开销也不到50%。

  - 对`-fdirectives-only`的支持有限。

    `-fdirectives-only`可以明显的改善预处理时间（降低~75%），但是某些情况下会失败（如某些情况下对`__COUNTER__`的使用）。

    但是目前已有的工具均采取简单的输出错误并本地重试（或放弃编译）的做法，实质上令`-fdirectives-only`成了一个要么所有`.h` / `.cc`不能依赖`__COUNTER__`、要么不能使用这一参数的二选一的境地。

  - 对本地并发度没有控制。

    无论是大量的（回落本地的）重试，又或是大量的链接，不对并发度加以限制容易导致OOM，并进而死机。

## 我们的工作

在调研了如上这些业界已有实现之后，我们设计了这一套`yadcc`，并做了如下一些事情：

- 分布式编译缓存：大仓库开发模式下**不同人之间可以共享缓存**。同时也可以优化少数情况下本地因为各种原因的重复编译。

  另外，我们在每台机器上会维护对应于缓存的布隆过滤器，**极大的减少了不会命中的缓存查询**（假阳性比率低于1/10000）。

- 高效的哈希、压缩算法：我们使用[zstd](https://github.com/facebook/zstd)进行压缩、[blake3](https://github.com/BLAKE3-team/BLAKE3)做密码学哈希（缓存Key及完整性校验等需要）、[xxHash](https://github.com/Cyan4973/xxHash)做普通哈希。这些算法相对于传统的`gzip`、`sha256`、`murmur`等有非常大的性能优势。考虑到**分布式编译场景下性能往往受制于本地CPU，精心选择算法可以带来明显的性能收益**。

  如GCC使用`-fdirectives-only`预处理大约100MB/s的输出，如果使用大约400MB/s的SHA，就会有20%的CPU浪费在哈希上。

  无独有偶，前不久刚刚发布的[ccache 4.0](https://ccache.dev/releasenotes.html#_ccache_4_0)也默认切换到了这些算法。

- 默认启用`-fdirectives-only`并静默重试：根据我们观察，大多数场景下`-fdirectives-only`都是可以使用的，这时候预处理速度有很大的收益。而少数情况下如果不能使用，我们会放弃这一参数静默重试，**同时兼顾性能和可用性**。

- 从调度器预取编译机、批量获取编译机：我们在监测到有编译任务之后，会预取一个编译机，这类似于我们在做性能优化时常用的[预取优化](https://en.wikipedia.org/wiki/Cache_prefetching)。同时，如果有多个任务堆积也会批量获取编译机。

  对于预取，考虑到通常编译任务生成速度（预处理速度）大于网络延时，因此通常都会有已经预取好的编译机可以直接提交任务。这有效的“异步化”了获取编译机的操作，避免了在关键路径向调度器发起RPC。

  同时，对于编译任务生成足够快的场景，通过批量获取编译机，可以降低每个任务花费在获取编译机这一RPC的“均摊成本”。

- 本地守护进程控制并发度：无论编译器进行何种操作（预处理、失败回落本地编译、链接），均会和本地守护进程协商，**在本地任务过多时排队等待**，避免导致本地过载死机。

此外，根据我们的实际场景，我们还做了如下一些事情：

- 实现了类似于P2P的“互助互利”的思想。

  `yadcc`集群中，各个机器即可以在编译时提交任务，也会在空闲时贡献一部分CPU来编译网络上的任务。

  **随着用户的增多，不但不会降低每个人分到的资源，反而会增大整个集群的处理能力**。

- 支持多版本编译器

  我们会在请求中携带编译器的哈希，调度器、守护进程、均会感知并针对不同版本的编译器分开处理（例如只会把任务指派到存在相应版本编译器的编译机。

  缓存部分，我们的缓存key也会计入编译器哈希，因此不同版本的编译结果也会区分处理。

  这**允许我们在一个集群中的多个版本的编译器和平共存**。也为后续继续升级编译器的“共存期”提供了可能。
